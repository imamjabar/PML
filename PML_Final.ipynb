{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PML_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmKsOkmlpfjo",
        "outputId": "96f03d7c-c2ec-45d7-bcd1-353968d19718"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zem73u-pm70",
        "outputId": "16d66162-c29a-442a-c7fe-5c56e0cabc0d"
      },
      "source": [
        "#Package untuk skor: Rouge, Bert_Score\n",
        "!pip install rouge\n",
        "!pip install bert_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n",
            "Collecting bert_score\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/fb/e63e7e231a79db0489dbf7e7d0ebfb279ccb3d8216aa0d133572f784f3fa/bert_score-0.3.9-py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert_score) (2.23.0)\n",
            "Collecting transformers>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert_score) (3.2.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from bert_score) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.9.0+cu102)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (2.10)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (4.5.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (3.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert_score) (2018.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->bert_score) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=3.0.0->bert_score) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0->bert_score) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0->bert_score) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0->bert_score) (7.1.2)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers, bert-score\n",
            "Successfully installed bert-score-0.3.9 huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEUYuObIqIS8"
      },
      "source": [
        "import random\n",
        "import queue as Queue\n",
        "import time\n",
        "from random import shuffle\n",
        "from threading import Thread\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "\n",
        "import shutil\n",
        "import collections\n",
        "\n",
        "import gc\n",
        "from rouge import Rouge\n",
        "import bert_score\n",
        "import argparse"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ6DA0zLdoCE"
      },
      "source": [
        "## Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNf8kfuUatHZ"
      },
      "source": [
        "# Pembuatan Vocab\n",
        "SENTENCE_START = '<s>'      \n",
        "SENTENCE_END = '</s>'       \n",
        "\n",
        "PAD_TOKEN = '[PAD]'         \n",
        "UNKNOWN_TOKEN = '[UNK]'     \n",
        "START_DECODING = '[START]'  \n",
        "STOP_DECODING = '[STOP]'   \n",
        "\n",
        "class Vocab(object):\n",
        "  def __init__(self, vocab_file, max_size):\n",
        "\n",
        "    self._word_to_id = {}                                                      \n",
        "    self._id_to_word = {}                                                    \n",
        "    self._count = 0                                                             \n",
        "\n",
        "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
        "      self._word_to_id[w] = self._count                                        \n",
        "      self._id_to_word[self._count] = w     \n",
        "      self._count += 1\n",
        "\n",
        "    with open(vocab_file, 'r') as vocab_f:\n",
        "      for line in vocab_f:\n",
        "        pieces = line.split()                                                  \n",
        "        if len(pieces) != 2:\n",
        "          print('Format vocab salah: %s\\n' % line)\n",
        "          continue\n",
        "        w = pieces[0]                                                     \n",
        "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:       \n",
        "          raise Exception('%s tidak boleh berada di dalam file vocab' % w)\n",
        "        if w in self._word_to_id:                                             \n",
        "          raise Exception('Duplicated word: %s' % w)       \n",
        "        self._word_to_id[w] = self._count                                      \n",
        "        self._id_to_word[self._count] = w                                       \n",
        "        self._count += 1                                                      \n",
        "        if max_size != 0 and self._count >= max_size:                          \n",
        "          print(\"max_size vocab: %i; %i words. Stopping reading.\" % (max_size, self._count))\n",
        "          break\n",
        "    print(\"Finish %i total words. Last word: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
        "\n",
        "  def word2id(self, word):\n",
        "    if word not in self._word_to_id:\n",
        "      return self._word_to_id[UNKNOWN_TOKEN]\n",
        "    return self._word_to_id[word]\n",
        "\n",
        "  def id2word(self, word_id):\n",
        "    if word_id not in self._id_to_word:\n",
        "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
        "    return self._id_to_word[word_id]\n",
        "\n",
        "  def size(self):\n",
        "    return self._count\n",
        "\n",
        "\n",
        "def example_generator(data_path, single_pass):\n",
        "  #Generates tf.Examples from data files.\n",
        "  while True:\n",
        "    filelist = glob.glob(data_path) \n",
        "    assert filelist, ('Error: Empty filelist at %s' % data_path)\n",
        "    if single_pass:\n",
        "      filelist = sorted(filelist)\n",
        "    else:\n",
        "      random.shuffle(filelist)\n",
        "    for f in filelist:\n",
        "      reader = open(f, 'rb')\n",
        "      while True:\n",
        "        len_bytes = reader.read(8)\n",
        "        if not len_bytes: break # finished reading this file\n",
        "        str_len = struct.unpack('q', len_bytes)[0]\n",
        "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
        "        yield example_pb2.Example.FromString(example_str)\n",
        "    if single_pass:\n",
        "      print(\"example_generator completed reading all datafiles. No more data.\")\n",
        "      break\n",
        "\n",
        "def article2ids(article_words, vocab):\n",
        "  #Map the article words to their ids. Also return a list of OOVs in the article.\n",
        "  ids = []\n",
        "  oovs = []\n",
        "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
        "  for w in article_words:\n",
        "    i = vocab.word2id(w)\n",
        "    if i == unk_id:\n",
        "      if w not in oovs:\n",
        "        oovs.append(w)\n",
        "      oov_num = oovs.index(w)\n",
        "      ids.append(vocab.size() + oov_num)\n",
        "    else:\n",
        "      ids.append(i)\n",
        "  return ids, oovs\n",
        "\n",
        "def abstract2ids(abstract_words, vocab, article_oovs):\n",
        "  #Map the abstract words to their ids. In-article OOVs are mapped to their temporary OOV numbers.\n",
        "  ids = []\n",
        "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
        "  for w in abstract_words:\n",
        "    i = vocab.word2id(w)\n",
        "    if i == unk_id:\n",
        "      if w in article_oovs:\n",
        "        vocab_idx = vocab.size() + article_oovs.index(w)\n",
        "        ids.append(vocab_idx)\n",
        "      else:\n",
        "        ids.append(unk_id)\n",
        "    else:\n",
        "      ids.append(i)\n",
        "  return ids\n",
        "\n",
        "\n",
        "def outputids2words(id_list, vocab, article_oovs):\n",
        "  #Maps output ids to words, including mapping in-article OOVs from their temporary ids to the original OOV string (applicable in pointer-generator mode).\n",
        "  words = []\n",
        "  for i in id_list:\n",
        "    try:\n",
        "      w = vocab.id2word(i) # might be [UNK]\n",
        "    except ValueError as e: # w is OOV\n",
        "      assert article_oovs is not None, \"Error: model produced a word ID that isn't in the vocabulary. This should not happen in baseline (no pointer-generator) mode\"\n",
        "      article_oov_idx = i - vocab.size()\n",
        "      try:\n",
        "        w = article_oovs[article_oov_idx]\n",
        "      except ValueError as e: # i doesn't correspond to an article oov\n",
        "        raise ValueError('Error: model produced word ID %i which corresponds to article OOV %i but this example only has %i article OOVs' % (i, article_oov_idx, len(article_oovs)))\n",
        "    words.append(w) \n",
        "  return words\n",
        "\n",
        "\n",
        "def abstract2sents(abstract):\n",
        "  #Splits abstract text from datafile into list of sentences.\n",
        "  cur = 0\n",
        "  sents = []\n",
        "  while True:\n",
        "    try:\n",
        "      start_p = abstract.index(SENTENCE_START, cur)\n",
        "      end_p = abstract.index(SENTENCE_END, start_p + 1)\n",
        "      cur = end_p + len(SENTENCE_END)\n",
        "      sents.append(abstract[start_p+len(SENTENCE_START):end_p])\n",
        "    except ValueError as e: # no more sentences\n",
        "      return sents"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2ofJnCAdmOw"
      },
      "source": [
        "## Batcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aMWC0kZqMVh"
      },
      "source": [
        "#Set Batcher\n",
        "max_batch_queue = 1000\n",
        "random.seed(2021)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8m2xBHBqR2Y"
      },
      "source": [
        "class Example(object):\n",
        "  #read article, abstrction and vocab and process them into batching-ready format\n",
        "\n",
        "  def __init__(self, article, abstract_sentences, vocab):\n",
        "    start_decoding = vocab.word2id(START_DECODING)\n",
        "    stop_decoding = vocab.word2id(STOP_DECODING) \n",
        "\n",
        "    # Process article\n",
        "    article_words = article.split()\n",
        "    if len(article_words) > max_enc_steps:             \n",
        "      article_words = article_words[ : max_enc_steps]   \n",
        "    self.enc_len = len(article_words)                  \n",
        "    self.enc_input = [vocab.word2id(w) for w in article_words]\n",
        "\n",
        "    # Process abstract\n",
        "    abstract = ' '.join(abstract_sentences)              \n",
        "    abstract_words = abstract.split()                      \n",
        "    abs_ids = [vocab.word2id(w) for w in abstract_words]  \n",
        "\n",
        "    # decoder input target sequence\n",
        "    self.dec_input, _ = self.get_dec_inp_targ_seqs(abs_ids, max_dec_steps, start_decoding, stop_decoding)\n",
        "    self.dec_len = len(self.dec_input)\n",
        "\n",
        "    #pointer\n",
        "    self.enc_input_extend_vocab, self.article_oovs = article2ids(article_words, vocab)\n",
        "\n",
        "    abs_ids_extend_vocab = abstract2ids(abstract_words, vocab, self.article_oovs)   \n",
        "\n",
        "    # decoder target sequence\n",
        "    _, self.target = self.get_dec_inp_targ_seqs(abs_ids_extend_vocab, max_dec_steps, start_decoding, stop_decoding)\n",
        "\n",
        "    # Store the original strings\n",
        "    self.original_article = article\n",
        "    self.original_abstract = abstract\n",
        "    self.original_abstract_sents = abstract_sentences\n",
        "\n",
        "  def get_dec_inp_targ_seqs(self, sequence, max_len, start_id, stop_id):\n",
        "    inp = [start_id] + sequence[:]\n",
        "    target = sequence[:]\n",
        "    if len(inp) > max_len: # truncate\n",
        "      inp = inp[:max_len]\n",
        "      target = target[:max_len] # no end_token\n",
        "    else: # no truncation\n",
        "      target.append(stop_id) # end token\n",
        "    assert len(inp) == len(target)\n",
        "    return inp, target\n",
        "\n",
        "  #padding\n",
        "  def pad_decoder_inp_targ(self, max_len, pad_id):\n",
        "    while len(self.dec_input) < max_len:\n",
        "      self.dec_input.append(pad_id)\n",
        "    while len(self.target) < max_len:\n",
        "      self.target.append(pad_id)\n",
        "\n",
        "  #padding\n",
        "  def pad_encoder_input(self, max_len, pad_id):\n",
        "    while len(self.enc_input) < max_len:\n",
        "      self.enc_input.append(pad_id)\n",
        "    while len(self.enc_input_extend_vocab) < max_len:\n",
        "      self.enc_input_extend_vocab.append(pad_id)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLjml-wl5yL6"
      },
      "source": [
        "class Batch(object):\n",
        "  def __init__(self, example_list, vocab, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.pad_id = vocab.word2id(PAD_TOKEN)\n",
        "    self.init_encoder_seq(example_list)\n",
        "    self.init_decoder_seq(example_list)\n",
        "    self.store_orig_strings(example_list)\n",
        "\n",
        "  def init_encoder_seq(self, example_list):\n",
        "    # Determine the maximum length of the encoder input sequence in this batch\n",
        "    max_enc_seq_len = max([ex.enc_len for ex in example_list])\n",
        "\n",
        "    # Pad the encoder input sequences up to the length of the longest sequence\n",
        "    for ex in example_list:\n",
        "      ex.pad_encoder_input(max_enc_seq_len, self.pad_id)\n",
        "\n",
        "    # Initialize the numpy arrays\n",
        "    self.enc_batch = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n",
        "    self.enc_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
        "    self.enc_padding_mask = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.float32)\n",
        "\n",
        "    # Fill in the numpy arrays\n",
        "    for i, ex in enumerate(example_list):\n",
        "      self.enc_batch[i, :] = ex.enc_input[:]\n",
        "      self.enc_lens[i] = ex.enc_len\n",
        "      for j in range(ex.enc_len):\n",
        "        self.enc_padding_mask[i][j] = 1\n",
        "\n",
        "    # Determine the max number of in-article OOVs in this batch\n",
        "    self.max_art_oovs = max([len(ex.article_oovs) for ex in example_list])\n",
        "    # Store the in-article OOVs themselves\n",
        "    self.art_oovs = [ex.article_oovs for ex in example_list]\n",
        "    # Store the version of the enc_batch that uses the article OOV ids\n",
        "    self.enc_batch_extend_vocab = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n",
        "    for i, ex in enumerate(example_list):\n",
        "      self.enc_batch_extend_vocab[i, :] = ex.enc_input_extend_vocab[:]\n",
        "\n",
        "  def init_decoder_seq(self, example_list):\n",
        "    # Pad the inputs and targets\n",
        "    for ex in example_list:\n",
        "      ex.pad_decoder_inp_targ(max_dec_steps, self.pad_id)\n",
        "\n",
        "    # Initialize the numpy arrays.\n",
        "    self.dec_batch = np.zeros((self.batch_size, max_dec_steps), dtype=np.int32)\n",
        "    self.target_batch = np.zeros((self.batch_size, max_dec_steps), dtype=np.int32)\n",
        "    self.dec_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
        "\n",
        "    # Fill in the numpy arrays\n",
        "    for i, ex in enumerate(example_list):\n",
        "      self.dec_batch[i, :] = ex.dec_input[:]\n",
        "      self.target_batch[i, :] = ex.target[:]\n",
        "      self.dec_lens[i] = ex.dec_lens\n",
        "\n",
        "  def store_orig_strings(self, example_list):\n",
        "    self.original_articles = [ex.original_article for ex in example_list]\n",
        "    self.original_abstracts = [ex.original_abstract for ex in example_list]\n",
        "    self.original_abstracts_sents = [ex.original_abstract_sents for ex in example_list]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxIlNd71cLXY"
      },
      "source": [
        "class Batcher(object):\n",
        "  BATCH_QUEUE_MAX = max_batch_queue\n",
        "\n",
        "  def __init__(self, data_path, vocab, mode, batch_size, single_pass):\n",
        "    self._data_path = data_path\n",
        "    self._vocab = vocab\n",
        "    self._single_pass = single_pass\n",
        "    self.mode = mode\n",
        "    self.batch_size = batch_size\n",
        "    # Initialize a queue of Batches waiting to be used, and a queue of Examples waiting to be batched\n",
        "    self._batch_queue = Queue.Queue(self.BATCH_QUEUE_MAX)\n",
        "    self._example_queue = Queue.Queue(self.BATCH_QUEUE_MAX * self.batch_size)\n",
        "\n",
        "    # Different settings depending on whether we're in single_pass mode or not\n",
        "    if single_pass:\n",
        "      self._num_example_q_threads = 1 # just one thread, read through the dataset just once\n",
        "      self._num_batch_q_threads = 1  # just one thread to batch examples\n",
        "      self._bucketing_cache_size = 1 # only load one batch's worth of examples before bucketing; this essentially means no bucketing\n",
        "      self._finished_reading = False # this will tell us when we're finished reading the dataset\n",
        "    else:\n",
        "      self._num_example_q_threads = 16\n",
        "      self._num_batch_q_threads = 4 \n",
        "      self._bucketing_cache_size = 100\n",
        "\n",
        "    # Start the threads that load the queues\n",
        "    self._example_q_threads = []\n",
        "    for _ in range(self._num_example_q_threads):\n",
        "      self._example_q_threads.append(Thread(target=self.fill_example_queue))\n",
        "      self._example_q_threads[-1].daemon = True\n",
        "      self._example_q_threads[-1].start()\n",
        "    self._batch_q_threads = []\n",
        "    for _ in range(self._num_batch_q_threads):\n",
        "      self._batch_q_threads.append(Thread(target=self.fill_batch_queue))\n",
        "      self._batch_q_threads[-1].daemon = True\n",
        "      self._batch_q_threads[-1].start()\n",
        "\n",
        "    # Start a thread that watches the other threads and restarts them if they're dead\n",
        "    if not single_pass:\n",
        "      self._watch_thread = Thread(target=self.watch_threads)\n",
        "      self._watch_thread.daemon = True\n",
        "      self._watch_thread.start()\n",
        "\n",
        "  def next_batch(self):\n",
        "    # If the batch queue is empty, print a warning\n",
        "    if self._batch_queue.qsize() == 0:\n",
        "      if self._single_pass and self._finished_reading:\n",
        "        tf.compat.v1.logging.info(\"Finished reading dataset in single_pass mode.\")\n",
        "        return None\n",
        "\n",
        "    batch = self._batch_queue.get() # get the next Batch\n",
        "    return batch\n",
        "\n",
        "  def fill_example_queue(self):\n",
        "    input_gen = self.text_generator(example_generator(self._data_path, self._single_pass))\n",
        "\n",
        "    while True:\n",
        "      try:\n",
        "        (article, abstract) = next(input_gen) # read the next example from file. article and abstract\n",
        "      except StopIteration:\n",
        "        tf.compat.v1.logging.info(\"The example generator for this example queue filling thread has exhausted data.\")\n",
        "        if self._single_pass:\n",
        "          tf.compat.v1.logging.info(\"single_pass mode is on, so we've finished reading dataset. This thread is stopping.\")\n",
        "          self._finished_reading = True\n",
        "          break\n",
        "        else:\n",
        "          raise Exception(\"single_pass mode is off but the example generator is out of data; error.\")\n",
        "\n",
        "      abstract_sentences = [abstract.strip()]\n",
        "      example = Example(article, abstract_sentences, self._vocab) \n",
        "      self._example_queue.put(example)\n",
        "\n",
        "  def fill_batch_queue(self):\n",
        "    while True:\n",
        "      if self.mode == 'decode':\n",
        "        ex = self._example_queue.get()\n",
        "        b = [ex for _ in range(self.batch_size)]\n",
        "        self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n",
        "      else:\n",
        "        inputs = []\n",
        "        for _ in range(self.batch_size * self._bucketing_cache_size):\n",
        "          inputs.append(self._example_queue.get())\n",
        "        inputs = sorted(inputs, key=lambda inp: inp.enc_len, reverse=True)\n",
        "\n",
        "        batches = []\n",
        "        for i in range(0, len(inputs), self.batch_size):\n",
        "          batches.append(inputs[i:i + self.batch_size])\n",
        "        if not self._single_pass:\n",
        "          shuffle(batches)\n",
        "        for b in batches:\n",
        "          self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n",
        "\n",
        "  def watch_threads(self):\n",
        "    while True:\n",
        "      tf.compat.v1.logging.info(\n",
        "        'Bucket queue size: %i, Input queue size: %i',\n",
        "        self._batch_queue.qsize(), self._example_queue.qsize())\n",
        "\n",
        "      time.sleep(60)\n",
        "      for idx,t in enumerate(self._example_q_threads):\n",
        "        if not t.is_alive():\n",
        "          tf.compat.v1.logging.error('Found example queue thread dead. Restarting.')\n",
        "          new_t = Thread(target=self.fill_example_queue)\n",
        "          self._example_q_threads[idx] = new_t\n",
        "          new_t.daemon = True\n",
        "          new_t.start()\n",
        "      for idx,t in enumerate(self._batch_q_threads):\n",
        "        if not t.is_alive(): \n",
        "          tf.compat.v1.logging.error('Found batch queue thread dead. Restarting.')\n",
        "          new_t = Thread(target=self.fill_batch_queue)\n",
        "          self._batch_q_threads[idx] = new_t\n",
        "          new_t.daemon = True\n",
        "          new_t.start()\n",
        "\n",
        "  def text_generator(self, example_generator):\n",
        "    while True:\n",
        "      e = next(example_generator)\n",
        "      try:\n",
        "        article_text = e.features.feature['article'].bytes_list.value[0] \n",
        "        abstract_text = e.features.feature['abstract'].bytes_list.value[0]\n",
        "        article_text = article_text.decode()\n",
        "        abstract_text = abstract_text.decode()\n",
        "      except ValueError:\n",
        "        tf.compat.v1.logging.error('Failed to get article or abstract from example')\n",
        "        continue\n",
        "      if len(article_text)==0:\n",
        "        continue\n",
        "      else:\n",
        "        yield (article_text, abstract_text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQELynd04C3G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-XD0ldddi9n"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyiKML6JqU55"
      },
      "source": [
        "def init_lstm_wt(lstm):     \n",
        "    for name, _ in lstm.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            wt = getattr(lstm, name)\n",
        "            wt.data = wt.data.uniform_(-rand_unif_init_mag, rand_unif_init_mag)\n",
        "        elif 'bias' in name:\n",
        "            bias = getattr(lstm, name)\n",
        "            n = bias.size(0)\n",
        "            start, end = n // 4, n // 2\n",
        "            bias.data = bias.data.fill_(0.)\n",
        "            bias.data[start:end].fill_(1.)\n",
        "\n",
        "def init_linear_wt(linear): \n",
        "    linear.weight.data = linear.weight.data.normal_(std=trunc_norm_init_std)\n",
        "    if linear.bias is not None:\n",
        "        linear.bias.data = linear.bias.data.normal_(std=trunc_norm_init_std)\n",
        "\n",
        "def init_wt_normal(wt):\n",
        "    wt.data = wt.data.normal_(std=trunc_norm_init_std)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=True) \n",
        "        init_lstm_wt(self.lstm)\n",
        "\n",
        "        self.reduce_h = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        init_linear_wt(self.reduce_h)\n",
        "        self.reduce_c = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        init_linear_wt(self.reduce_c)\n",
        "\n",
        "    def forward(self, x, seq_lens):\n",
        "        packed = pack_padded_sequence(x, seq_lens, batch_first=True)\n",
        "        enc_out, enc_hid = self.lstm(packed)                           \n",
        "        enc_out,_ = pad_packed_sequence(enc_out, batch_first=True)      \n",
        "        enc_out = enc_out.contiguous()                             \n",
        "        h, c = enc_hid                                           \n",
        "        h = T.cat(list(h), dim=1)                               \n",
        "        c = T.cat(list(c), dim=1)\n",
        "        h_reduced = F.relu(self.reduce_h(h))                     \n",
        "        c_reduced = F.relu(self.reduce_c(c))\n",
        "        return enc_out, (h_reduced, c_reduced)    \n",
        "\n",
        "class encoder_attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(encoder_attention, self).__init__()\n",
        "        self.W_h = nn.Linear(hidden_dim * 2, hidden_dim * 2, bias=False)   \n",
        "        self.W_s = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
        "        self.v = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "\n",
        "    def forward(self, st_hat, h, enc_padding_mask, sum_temporal_srcs):           \n",
        "        et = self.W_h(h)\n",
        "        dec_fea = self.W_s(st_hat).unsqueeze(1)\n",
        "        et = et + dec_fea\n",
        "        et = T.tanh(et)\n",
        "        et = self.v(et).squeeze(2)\n",
        "\n",
        "        if intra_encoder:\n",
        "            exp_et = T.exp(et)\n",
        "            if sum_temporal_srcs is None:\n",
        "                et1 = exp_et\n",
        "                sum_temporal_srcs  = get_cuda(T.FloatTensor(et.size()).fill_(1e-10)) + exp_et      \n",
        "            else:\n",
        "                et1 = exp_et/sum_temporal_srcs\n",
        "                sum_temporal_srcs = sum_temporal_srcs + exp_et\n",
        "        else:\n",
        "            et1 = F.softmax(et, dim=1)\n",
        "\n",
        "        at = et1 * enc_padding_mask\n",
        "        normalization_factor = at.sum(1, keepdim=True)\n",
        "        at = at / normalization_factor\n",
        "\n",
        "        at = at.unsqueeze(1)          \n",
        "        # encoder context vector\n",
        "        ct_e = T.bmm(at, h)                   \n",
        "        ct_e = ct_e.squeeze(1)\n",
        "        at = at.squeeze(1)\n",
        "\n",
        "        return ct_e, at, sum_temporal_srcs\n",
        "\n",
        "class decoder_attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(decoder_attention, self).__init__()\n",
        "        if intra_decoder:\n",
        "            self.W_prev = nn.Linear(hidden_dim, hidden_dim, bias=False)        #weight\n",
        "            self.W_s = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, s_t, prev_s):\n",
        "        #intra_decoder attention\n",
        "        if intra_decoder is False:\n",
        "            ct_d = get_cuda(T.zeros(s_t.size()))\n",
        "        elif prev_s is None:\n",
        "            ct_d = get_cuda(T.zeros(s_t.size()))\n",
        "            prev_s = s_t.unsqueeze(1)\n",
        "        else:\n",
        "            et = self.W_prev(prev_s)\n",
        "            dec_fea = self.W_s(s_t).unsqueeze(1)\n",
        "            et = et + dec_fea\n",
        "            et = T.tanh(et)\n",
        "            et = self.v(et).squeeze(2)\n",
        "\n",
        "            at = F.softmax(et, dim=1).unsqueeze(1)\n",
        "            ct_d = T.bmm(at, prev_s).squeeze(1)\n",
        "            prev_s = T.cat([prev_s, s_t.unsqueeze(1)], dim=1)\n",
        "\n",
        "        #decoder context vector, previous decoder hidden states\n",
        "        return ct_d, prev_s \n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.enc_attention = encoder_attention()\n",
        "        self.dec_attention = decoder_attention()\n",
        "        self.x_context = nn.Linear(hidden_dim*2 + emb_dim, emb_dim)\n",
        "\n",
        "        self.lstm = nn.LSTMCell(emb_dim, hidden_dim)\n",
        "        init_lstm_wt(self.lstm)\n",
        "\n",
        "        self.p_gen_linear = nn.Linear(hidden_dim * 5 + emb_dim, 1)\n",
        "\n",
        "        self.V = nn.Linear(hidden_dim*4, hidden_dim)\n",
        "        self.V1 = nn.Linear(hidden_dim, vocab_size)\n",
        "        init_linear_wt(self.V1)\n",
        "\n",
        "    def forward(self, x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s):\n",
        "        x = self.x_context(T.cat([x_t, ct_e], dim=1))\n",
        "        s_t = self.lstm(x, s_t)\n",
        "\n",
        "        dec_h, dec_c = s_t\n",
        "        st_hat = T.cat([dec_h, dec_c], dim=1)\n",
        "        ct_e, attn_dist, sum_temporal_srcs = self.enc_attention(st_hat, enc_out, enc_padding_mask, sum_temporal_srcs)\n",
        "\n",
        "        #intra-decoder attention\n",
        "        ct_d, prev_s = self.dec_attention(dec_h, prev_s)\n",
        "\n",
        "        p_gen = T.cat([ct_e, ct_d, st_hat, x], 1)\n",
        "        p_gen = self.p_gen_linear(p_gen)\n",
        "        p_gen = T.sigmoid(p_gen)\n",
        "\n",
        "        out = T.cat([dec_h, ct_e, ct_d], dim=1)\n",
        "        out = self.V(out)\n",
        "        out = self.V1(out)\n",
        "        vocab_dist = F.softmax(out, dim=1)\n",
        "        vocab_dist = p_gen * vocab_dist\n",
        "        attn_dist_ = (1 - p_gen) * attn_dist\n",
        "\n",
        "        # pointer mechanism \n",
        "        if extra_zeros is not None:\n",
        "            vocab_dist = T.cat([vocab_dist, extra_zeros], dim=1)\n",
        "        final_dist = vocab_dist.scatter_add(1, enc_batch_extend_vocab, attn_dist_)\n",
        "\n",
        "        return final_dist, s_t, ct_e, sum_temporal_srcs, prev_s\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.embeds = nn.Embedding(vocab_size, emb_dim)\n",
        "        init_wt_normal(self.embeds.weight)\n",
        "\n",
        "        self.encoder = get_cuda(self.encoder)\n",
        "        self.decoder = get_cuda(self.decoder)\n",
        "        self.embeds = get_cuda(self.embeds)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4UJYfSIqZ4A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icGmWO9DeQhw"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgVwdl-yqirR"
      },
      "source": [
        "def get_cuda(tensor):\n",
        "    if T.cuda.is_available():\n",
        "        tensor = tensor.cuda()\n",
        "    return tensor\n",
        "\n",
        "def get_enc_data(batch):\n",
        "    batch_size = len(batch.enc_lens)\n",
        "    enc_batch = T.from_numpy(batch.enc_batch).long()\n",
        "    enc_padding_mask = T.from_numpy(batch.enc_padding_mask).float()\n",
        "\n",
        "    enc_lens = batch.enc_lens\n",
        "    ct_e = T.zeros(batch_size, 2*   hidden_dim)   #config.hidden_dim\n",
        "\n",
        "    enc_batch = get_cuda(enc_batch)\n",
        "    enc_padding_mask = get_cuda(enc_padding_mask)\n",
        "    ct_e = get_cuda(ct_e)\n",
        "\n",
        "    enc_batch_extend_vocab = None\n",
        "    if batch.enc_batch_extend_vocab is not None:\n",
        "        enc_batch_extend_vocab = T.from_numpy(batch.enc_batch_extend_vocab).long()\n",
        "        enc_batch_extend_vocab = get_cuda(enc_batch_extend_vocab)\n",
        "\n",
        "    extra_zeros = None\n",
        "    if batch.max_art_oovs > 0:\n",
        "        extra_zeros = T.zeros(batch_size, batch.max_art_oovs)\n",
        "        extra_zeros = get_cuda(extra_zeros)\n",
        "\n",
        "    return enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, ct_e\n",
        "\n",
        "\n",
        "def get_dec_data(batch):\n",
        "    dec_batch = T.from_numpy(batch.dec_batch).long()\n",
        "    dec_lens = batch.dec_lens\n",
        "    max_dec_len = np.max(dec_lens)\n",
        "    dec_lens = T.from_numpy(batch.dec_lens).float()\n",
        "\n",
        "    target_batch = T.from_numpy(batch.target_batch).long()\n",
        "    dec_batch = get_cuda(dec_batch)\n",
        "    dec_lens = get_cuda(dec_lens)\n",
        "    target_batch = get_cuda(target_batch)\n",
        "\n",
        "    return dec_batch, max_dec_len, dec_lens, target_batch"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbz4nugUecGH"
      },
      "source": [
        "## Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuzdGLLqqjWK"
      },
      "source": [
        "#Beam Search\n",
        "class Beam(object):\n",
        "    def __init__(self, start_id, end_id, unk_id, hidden_state, context):\n",
        "        h,c = hidden_state\n",
        "        self.tokens = T.LongTensor(beam_size,1).fill_(start_id)\n",
        "        self.scores = T.FloatTensor(beam_size,1).fill_(-30)\n",
        "        self.tokens, self.scores = get_cuda(self.tokens), get_cuda(self.scores)\n",
        "        self.scores[0][0] = 0\n",
        "        self.hid_h = h.unsqueeze(0).repeat(beam_size, 1)\n",
        "        self.hid_c = c.unsqueeze(0).repeat(beam_size, 1)\n",
        "        self.context = context.unsqueeze(0).repeat(beam_size, 1)\n",
        "        self.sum_temporal_srcs = None\n",
        "        self.prev_s = None\n",
        "        self.done = False\n",
        "        self.end_id = end_id\n",
        "        self.unk_id = unk_id\n",
        "\n",
        "    def get_current_state(self):\n",
        "        tokens = self.tokens[:,-1].clone()\n",
        "        for i in range(len(tokens)):\n",
        "            if tokens[i].item() >= vocab_size:\n",
        "                tokens[i] = self.unk_id\n",
        "        return tokens\n",
        "\n",
        "    def advance(self, prob_dist, hidden_state, context, sum_temporal_srcs, prev_s):\n",
        "        #Run beam search\n",
        "        n_extended_vocab = prob_dist.size(1)\n",
        "        h, c = hidden_state\n",
        "        log_probs = T.log(prob_dist+eps)\n",
        "\n",
        "        scores = log_probs + self.scores\n",
        "        scores = scores.view(-1,1)\n",
        "        best_scores, best_scores_id = T.topk(input=scores, k=beam_size, dim=0)\n",
        "        self.scores = best_scores\n",
        "        beams_order = best_scores_id.squeeze(1)//n_extended_vocab\n",
        "        best_words = best_scores_id%n_extended_vocab\n",
        "        self.hid_h = h[beams_order]\n",
        "        self.hid_c = c[beams_order]\n",
        "        self.context = context[beams_order]\n",
        "        if sum_temporal_srcs is not None:\n",
        "            self.sum_temporal_srcs = sum_temporal_srcs[beams_order]\n",
        "        if prev_s is not None:\n",
        "            self.prev_s = prev_s[beams_order]\n",
        "        self.tokens = self.tokens[beams_order]\n",
        "        self.tokens = T.cat([self.tokens, best_words], dim=1)\n",
        "\n",
        "        #End condition is when top-of-beam is EOS.\n",
        "        if best_words[0][0] == self.end_id:\n",
        "            self.done = True\n",
        "\n",
        "    def get_best(self):\n",
        "        best_token = self.tokens[0].cpu().numpy().tolist()\n",
        "        try:\n",
        "            end_idx = best_token.index(self.end_id)\n",
        "        except ValueError:\n",
        "            end_idx = len(best_token)\n",
        "        best_token = best_token[1:end_idx]\n",
        "        return best_token\n",
        "\n",
        "    def get_all(self):\n",
        "        all_tokens = []\n",
        "        for i in range(len(self.tokens)):\n",
        "            all_tokens.append(self.tokens[i].cpu().numpy())\n",
        "        return all_tokens\n",
        "\n",
        "def beam_search(enc_hid, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, model, start_id, end_id, unk_id):\n",
        "    batch_size = len(enc_hid[0])\n",
        "    beam_idx = T.LongTensor(list(range(batch_size)))\n",
        "    #For each example in batch, create Beam object\n",
        "    beams = [Beam(start_id, end_id, unk_id, (enc_hid[0][i], enc_hid[1][i]), ct_e[i]) for i in range(batch_size)]\n",
        "    n_rem = batch_size\n",
        "    sum_temporal_srcs = None\n",
        "    prev_s = None\n",
        "\n",
        "    for t in range(max_dec_steps):\n",
        "        x_t = T.stack(\n",
        "            [beam.get_current_state() for beam in beams if beam.done == False]\n",
        "        ).contiguous().view(-1)\n",
        "        x_t = model.embeds(x_t)\n",
        "\n",
        "        dec_h = T.stack(\n",
        "            [beam.hid_h for beam in beams if beam.done == False]\n",
        "        ).contiguous().view(-1,hidden_dim)\n",
        "        dec_c = T.stack(\n",
        "            [beam.hid_c for beam in beams if beam.done == False]\n",
        "        ).contiguous().view(-1,hidden_dim)\n",
        "\n",
        "        ct_e = T.stack(\n",
        "            [beam.context for beam in beams if beam.done == False]\n",
        "        ).contiguous().view(-1,2*hidden_dim)\n",
        "\n",
        "        if sum_temporal_srcs is not None:\n",
        "            sum_temporal_srcs = T.stack(\n",
        "                [beam.sum_temporal_srcs for beam in beams if beam.done == False]\n",
        "            ).contiguous().view(-1, enc_out.size(1))\n",
        "\n",
        "        if prev_s is not None:\n",
        "            prev_s = T.stack(\n",
        "                [beam.prev_s for beam in beams if beam.done == False]\n",
        "            ).contiguous().view(-1, t, hidden_dim)\n",
        "\n",
        "        s_t = (dec_h, dec_c)\n",
        "        enc_out_beam = enc_out[beam_idx].view(n_rem,-1).repeat(1, beam_size).view(-1, enc_out.size(1), enc_out.size(2))\n",
        "        enc_pad_mask_beam = enc_padding_mask[beam_idx].repeat(1, beam_size).view(-1, enc_padding_mask.size(1))\n",
        "\n",
        "        extra_zeros_beam = None\n",
        "        if extra_zeros is not None:\n",
        "            extra_zeros_beam = extra_zeros[beam_idx].repeat(1, beam_size).view(-1, extra_zeros.size(1))\n",
        "        enc_extend_vocab_beam = enc_batch_extend_vocab[beam_idx].repeat(1, beam_size).view(-1, enc_batch_extend_vocab.size(1))\n",
        "\n",
        "        final_dist, (dec_h, dec_c), ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out_beam, enc_pad_mask_beam, ct_e, extra_zeros_beam, enc_extend_vocab_beam, sum_temporal_srcs, prev_s)\n",
        "\n",
        "        final_dist = final_dist.view(n_rem, beam_size, -1)\n",
        "        dec_h = dec_h.view(n_rem, beam_size, -1)\n",
        "        dec_c = dec_c.view(n_rem, beam_size, -1)\n",
        "        ct_e = ct_e.view(n_rem, beam_size, -1)\n",
        "\n",
        "        if sum_temporal_srcs is not None:\n",
        "            sum_temporal_srcs = sum_temporal_srcs.view(n_rem, beam_size, -1)\n",
        "\n",
        "        if prev_s is not None:\n",
        "            prev_s = prev_s.view(n_rem, beam_size, -1, hidden_dim)\n",
        "\n",
        "        # For all the active beams, perform beam search\n",
        "        active = []\n",
        "        for i in range(n_rem):\n",
        "            b = beam_idx[i].item()\n",
        "            beam = beams[b]\n",
        "            if beam.done:\n",
        "                continue\n",
        "\n",
        "            sum_temporal_srcs_i = prev_s_i = None\n",
        "            if sum_temporal_srcs is not None:\n",
        "                sum_temporal_srcs_i = sum_temporal_srcs[i]\n",
        "            if prev_s is not None:\n",
        "                prev_s_i = prev_s[i]\n",
        "            beam.advance(final_dist[i], (dec_h[i], dec_c[i]), ct_e[i], sum_temporal_srcs_i, prev_s_i)\n",
        "            if beam.done == False:\n",
        "                active.append(b)\n",
        "\n",
        "        if len(active) == 0:\n",
        "            break\n",
        "\n",
        "        beam_idx = T.LongTensor(active)\n",
        "        n_rem = len(beam_idx)\n",
        "\n",
        "    predicted_words = []\n",
        "    for beam in beams:\n",
        "        predicted_words.append(beam.get_best())\n",
        "\n",
        "    return predicted_words"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JHrzajFfORq"
      },
      "source": [
        "## Evaluation -  Rouge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ous3jcoqpTT"
      },
      "source": [
        "#Evaluation with Rouge\n",
        "def get_cuda(tensor):\n",
        "    if T.cuda.is_available():\n",
        "        tensor = tensor.cuda()\n",
        "    return tensor\n",
        "\n",
        "class Evaluate_r(object):\n",
        "    def __init__(self, data_path, opt, batch_size):\n",
        "        self.vocab = Vocab(vocab_path, vocab_size)\n",
        "        self.batcher = Batcher(data_path, self.vocab, mode='eval',\n",
        "                               batch_size=batch_size, single_pass=True)\n",
        "        self.opt = opt\n",
        "        time.sleep(5)\n",
        "\n",
        "    def setup_valid(self):\n",
        "        self.model = Model()\n",
        "        self.model = get_cuda(self.model)\n",
        "        checkpoint = T.load(os.path.join(save_model_path, self.opt.load_model))\n",
        "        self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
        "\n",
        "\n",
        "    def print_original_predicted(self, decoded_sents, ref_sents, article_sents, loadfile):\n",
        "        filename = \"test_\"+loadfile.split(\".\")[0]+\".txt\"\n",
        "    \n",
        "        with open(os.path.join(save_example_path,filename), \"w\") as f:\n",
        "            for i in range(len(decoded_sents)):\n",
        "                f.write(\"article: \"+article_sents[i] + \"\\n\")\n",
        "                f.write(\"ref: \" + ref_sents[i] + \"\\n\")\n",
        "                f.write(\"dec: \" + decoded_sents[i] + \"\\n\\n\")\n",
        "\n",
        "    def evaluate_batch(self, print_sents = False):\n",
        "\n",
        "        self.setup_valid()\n",
        "        batch = self.batcher.next_batch()\n",
        "        start_id = self.vocab.word2id(START_DECODING)\n",
        "        end_id = self.vocab.word2id(STOP_DECODING)\n",
        "        unk_id = self.vocab.word2id(UNKNOWN_TOKEN)\n",
        "        decoded_sents = []\n",
        "        ref_sents = []\n",
        "        article_sents = []\n",
        "        rouge = Rouge()\n",
        "        while batch is not None:\n",
        "            enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, ct_e = get_enc_data(batch)\n",
        "\n",
        "            with T.autograd.no_grad():\n",
        "                enc_batch = self.model.embeds(enc_batch)\n",
        "                enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
        "\n",
        "            #Summarization\n",
        "            with T.autograd.no_grad():\n",
        "                pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, self.model, start_id, end_id, unk_id)\n",
        "\n",
        "            for i in range(len(pred_ids)):\n",
        "                decoded_words = outputids2words(pred_ids[i], self.vocab, batch.art_oovs[i])\n",
        "                if len(decoded_words) < 2:\n",
        "                    decoded_words = \"xxx\"\n",
        "                else:\n",
        "                    decoded_words = \" \".join(decoded_words)\n",
        "                decoded_sents.append(decoded_words)\n",
        "                abstract = batch.original_abstracts[i]\n",
        "                article = batch.original_articles[i]\n",
        "                ref_sents.append(abstract)\n",
        "                article_sents.append(article)\n",
        "            batch = self.batcher.next_batch()\n",
        "        load_file = self.opt.load_model\n",
        "\n",
        "        if print_sents:\n",
        "            self.print_original_predicted(decoded_sents, ref_sents, article_sents, load_file)\n",
        "\n",
        "        scores = rouge.get_scores(decoded_sents, ref_sents, avg = True)\n",
        "        if self.opt.task == \"test\":\n",
        "            print(load_file, \"scores:\", scores)\n",
        "        else:\n",
        "            rouge_l = scores[\"rouge-l\"][\"f\"]\n",
        "            print(load_file, \"rouge_l:\", \"%.4f\" % rouge_l)\n",
        "            print('Scores:', scores)\n",
        "        \n",
        "        #Rouge 1\n",
        "        rouge_1 = scores['rouge-1']['f']\n",
        "        #Rouge 2\n",
        "        rouge_2 = scores['rouge-2']['f']\n",
        "        #Rouge L\n",
        "        rouge_l = scores['rouge-l']['f']\n",
        "\n",
        "        return rouge_1, rouge_2, rouge_l"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrgaAauyfbR-"
      },
      "source": [
        "##Evaluation - Bert Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KCqyTkyquTy"
      },
      "source": [
        "def get_cuda(tensor):\n",
        "    if T.cuda.is_available():\n",
        "        tensor = tensor.cuda()\n",
        "    return tensor\n",
        "\n",
        "class Evaluate_b(object):\n",
        "    def __init__(self, data_path, opt, batch_size):\n",
        "        self.vocab = Vocab(vocab_path, vocab_size)\n",
        "        self.batcher = Batcher(data_path, self.vocab, mode='eval',\n",
        "                               batch_size=batch_size, single_pass=True)\n",
        "        self.opt = opt\n",
        "\n",
        "        time.sleep(5)\n",
        "\n",
        "    def setup_valid(self):\n",
        "        self.model = Model()\n",
        "        self.model = get_cuda(self.model)\n",
        "        checkpoint = T.load(os.path.join(save_model_path, self.opt.load_model))\n",
        "        self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
        "\n",
        "\n",
        "    def print_original_predicted(self, decoded_sents, ref_sents, article_sents, loadfile):\n",
        "        filename = \"test_\"+loadfile.split(\".\")[0]+\".txt\"\n",
        "    \n",
        "        with open(os.path.join(save_example_path,filename), \"w\") as f:\n",
        "            for i in range(len(decoded_sents)):\n",
        "                f.write(\"article: \"+article_sents[i] + \"\\n\")\n",
        "                f.write(\"ref: \" + ref_sents[i] + \"\\n\")\n",
        "                f.write(\"dec: \" + decoded_sents[i] + \"\\n\\n\")\n",
        "\n",
        "    def evaluate_batch(self, print_sents = False):\n",
        "\n",
        "        self.setup_valid()\n",
        "        batch = self.batcher.next_batch()\n",
        "        start_id = self.vocab.word2id(START_DECODING)\n",
        "        end_id = self.vocab.word2id(STOP_DECODING)\n",
        "        unk_id = self.vocab.word2id(UNKNOWN_TOKEN)\n",
        "        decoded_sents = []\n",
        "        ref_sents = []\n",
        "        article_sents = []\n",
        "        while batch is not None:\n",
        "            enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, ct_e = get_enc_data(batch)\n",
        "\n",
        "            with T.autograd.no_grad():\n",
        "                enc_batch = self.model.embeds(enc_batch)\n",
        "                enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
        "\n",
        "            #Summarization\n",
        "            with T.autograd.no_grad():\n",
        "                pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, self.model, start_id, end_id, unk_id)\n",
        "\n",
        "            for i in range(len(pred_ids)):\n",
        "                decoded_words = outputids2words(pred_ids[i], self.vocab, batch.art_oovs[i])\n",
        "                if len(decoded_words) < 2:\n",
        "                    decoded_words = \"xxx\"\n",
        "                else:\n",
        "                    decoded_words = \" \".join(decoded_words)\n",
        "                decoded_sents.append(decoded_words)\n",
        "                abstract = batch.original_abstracts[i]\n",
        "                article = batch.original_articles[i]\n",
        "                ref_sents.append(abstract)\n",
        "                article_sents.append(article)\n",
        "\n",
        "            batch = self.batcher.next_batch()\n",
        "\n",
        "        load_file = self.opt.load_model\n",
        "        _,_,f = bert_score.score(decoded_sents, ref_sents, lang='en', verbose = False)\n",
        "\n",
        "        print('{}  score:{}'.format(load_file, f.mean()))\n",
        "        gc.collect()\n",
        "        \n",
        "        if print_sents:\n",
        "            self.print_original_predicted(decoded_sents, ref_sents, article_sents, load_file)\n",
        "        return f.mean()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0fSRAyQrCYY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CUi_sBLfnaU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9hRHsNIflrD"
      },
      "source": [
        "# EXPERIMENT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6LIsdqOrEYf"
      },
      "source": [
        "#Model hasil running\n",
        "model_folder = 'drive/My Drive/PML/CNNDM_Test'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sex4eMQLra8a",
        "outputId": "b53a4ba1-a6c0-4a66-b442-f7e44dd84b54"
      },
      "source": [
        "!ls 'drive/My Drive/PML/CNNDM_Test'"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model  test  vocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCNjT0phwAWi"
      },
      "source": [
        "test_data_path = (model_folder + '/test/test_*')\n",
        "vocab_path = \t(model_folder+ '/vocab')\t  \n",
        "\n",
        "# Hyperparameters\n",
        "hidden_dim = 400\n",
        "emb_dim = 200\n",
        "batch_size = 50\n",
        "max_enc_steps = 400\t\t\n",
        "max_dec_steps = 100\t\t\n",
        "beam_size = 5\n",
        "min_dec_steps= 3\n",
        "vocab_size = 30000 \n",
        "\n",
        "lr = 0.001\n",
        "rand_unif_init_mag = 0.02\n",
        "trunc_norm_init_std = 1e-4\n",
        "\n",
        "eps = 1e-12\n",
        "max_iterations = 10000\n",
        "max_batch_queue = 100\n",
        "\n",
        "intra_encoder = True\n",
        "intra_decoder = True"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmOpFpUNgA8v"
      },
      "source": [
        "## Testing RL Reward Rouge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkFalCjdwQxn"
      },
      "source": [
        "#RL reward rouge:\n",
        "save_model_path =  (model_folder + '/model/')           \n",
        "model_name_for_r_testing = 'RL(r).tar'\n",
        "model_name_for_b_testing = 'RL(r).tar' "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgvSILmlgUui",
        "outputId": "f5a5550b-cd4a-4a4b-ddca-d8a05172e47f"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n",
        "    parser.add_argument(\"--start_from\", type=str, default=None)             \n",
        "    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)  \n",
        "\n",
        "    opt, unknown = parser.parse_known_args()\n",
        "\n",
        "    #Eval\n",
        "    eval_r = Evaluate_r(test_data_path, opt, batch_size)  \n",
        "    r_rlr_1, r_rlr_2, r_rlr_L = eval_r.evaluate_batch()\n",
        "\n",
        "    opt.load_model= model_name_for_b_testing    \n",
        "    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n",
        "    rlr_bert_testing = eval_b.evaluate_batch()\n",
        "\n",
        "    print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_rlr_1, r_rlr_2, r_rlr_L, rlr_bert_testing))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n",
            "Finished constructing vocabulary of 30000 total words. Last word added: moles\n",
            "example_generator completed reading all datafiles. No more data.\n",
            "INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n",
            "INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n",
            "\n",
            "INFO:tensorflow:Finished reading dataset in single_pass mode.\n",
            "RL(r) testing.tar scores: {'rouge-1': {'f': 0.2792071217548919, 'p': 0.23658517128148124, 'r': 0.35713651610061982}, 'rouge-2': {'f': 0.10220711289029155, 'p': 0.0865442180911029, 'r': 0.12471812648659011}, 'rouge-l': {'f': 0.3392499018214784, 'p': 0.34383582711081823, 'r': 0.3558461987570033}}\n",
            "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n",
            "\n",
            "\n",
            "max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n",
            "Finished constructing vocabulary of 30000 total words. Last word added: moles\n",
            "example_generator completed reading all datafiles. No more data.\n",
            "INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n",
            "INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n",
            "INFO:tensorflow:Finished reading dataset in single_pass mode.\n",
            "RL(r) testing.tar  score:0.8132711878193782\n",
            "ROUGE_1: 0.2792 ; ROUGE_2: 0.1022; ROUGE_L: 0.3392; BERTScore: 0.8132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-37eymOOiQci"
      },
      "source": [
        "## Testing RL Reward BertScore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CARQivmiM8i"
      },
      "source": [
        "save_model_path =  (model_folder + '/model')      \n",
        "model_name_for_r_testing = 'RL(b) for ROUGE.tar'\n",
        "model_name_for_b_testing = 'RL(b) for BERT.tar'"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0cwWggNi0EV",
        "outputId": "0f5f0c31-fd61-4fd6-a884-d8adaae0d3d3"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--task\", type=str, default=\"test\", choices=[\"validate\",\"test\"])\n",
        "    parser.add_argument(\"--start_from\", type=str, default=None)             \n",
        "    parser.add_argument(\"--load_model\", type=str, default=model_name_for_r_testing)  \n",
        "    \n",
        "    opt, unknown = parser.parse_known_args()\n",
        "\n",
        "    #Eval\n",
        "    eval_r = Evaluate_r(test_data_path, opt, batch_size)        \n",
        "    r_rlb_1, r_rlb_2, r_rlb_L = eval_r.evaluate_batch()\n",
        "\n",
        "    opt.load_model= model_name_for_b_testing    \n",
        "    eval_b = Evaluate_b(test_data_path, opt, batch_size)\n",
        "    rlb_bert_testing = eval_b.evaluate_batch()\n",
        "    print('ROUGE_1: {:.4f} ; ROUGE_2: {:.4f}; ROUGE_L: {:.4f}; BERTScore: {:.4f}'.format(r_rlb_1, r_rlb_2, r_rlb_L, rlb_bert_testing))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n",
            "Finished constructing vocabulary of 30000 total words. Last word added: moles\n",
            "example_generator completed reading all datafiles. No more data.\n",
            "INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n",
            "INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n",
            "\n",
            "INFO:tensorflow:Finished reading dataset in single_pass mode.\n",
            "RL(b) for ROUGE testing.tar scores: {'rouge-1': {'f': 0.31452751289110214, 'p': 0.33677212860126349, 'r': 0.31581721909211306}, 'rouge-2': {'f': 0.12831026385390516, 'p': 0.13149961789026291, 'r': 0.12681581928114970}, 'rouge-l': {'f': 0.31713614191827518, 'p': 0.36232148112912141, 'r': 0.33258529182790112}}\n",
            "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n",
            "\n",
            "\n",
            "max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n",
            "Finished constructing vocabulary of 30000 total words. Last word added: moles\n",
            "example_generator completed reading all datafiles. No more data.\n",
            "INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n",
            "INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n",
            "INFO:tensorflow:Finished reading dataset in single_pass mode.\n",
            "RL(b) for BERT testing.tar  score:0.8384511827190346\n",
            "ROUGE_1: 0.3145 ; ROUGE_2: 0.1283; ROUGE_L: 0.3171; BERTScore: 0.8384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJwMKwFviYTa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmBswDOZkxsY"
      },
      "source": [
        "## Hasil"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "qR1HwdoIiYYM",
        "outputId": "4c71d176-1800-49e0-f207-b02f4a916468"
      },
      "source": [
        "data_frame = {'ROUGE 1':[ r_rlr_1, r_rlb_1], 'ROUGE 2':[r_rlr_2, r_rlb_2], 'ROUGE L':[r_rlr_L, r_rlb_L], \n",
        "              'BERTScore':[float(rlr_bert_testing), float(rlb_bert_testing)]}\n",
        "df = pd.DataFrame(data_frame, index= ['RL(r)', 'RL(b)']).round(4)\n",
        "df"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ROUGE 1</th>\n",
              "      <th>ROUGE 2</th>\n",
              "      <th>ROUGE L</th>\n",
              "      <th>BERTScore</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>RL(r)</th>\n",
              "      <td>0.2792</td>\n",
              "      <td>0.1022</td>\n",
              "      <td>0.3392</td>\n",
              "      <td>0.8132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RL(b)</th>\n",
              "      <td>0.3145</td>\n",
              "      <td>0.1283</td>\n",
              "      <td>0.3171</td>\n",
              "      <td>0.8384</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ROUGE 1  ROUGE 2  ROUGE L  BERTScore\n",
              "RL(r)   0.2792   0.1022   0.3392     0.8132\n",
              "RL(b)   0.3145   0.1283   0.3171     0.8384"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}